Clustering for dataset exploration

Unsupervised learning:
- Unsupervised learning finds patterns in data 
- Eg. clustering customers by their purchases 
- Compressing the data using purchase patterns (dimension reduction)

Supervised vs unsupervised learning:
- Supervised learning finds patterns for a prediction task
- eg. classify tumors as benign or cancerous (labels)
- Unsupervised learning finds patterns in data 
- .. but without a specific prediction task in mind 

Arrays, features and samples:
- 2D NumPy array 
- Columns are measurements (the features)
- Rows represent iris plants (the samples)

 Iris data is 4-dimensional:
 - Iris samples are points in 4 dimensional space 
 - Dimension = number of features 
 - Dimension too high to visualize
 - .. unsupervised learning gives insight

 K-means clustering:
 - Finds clusters in smaples 
 - Number of clusters must be specified 
 - implemented in sklearn

 KMeans in sklearn:
 from sklearn.cluster import KMeans
 model = KMeans(n_clusters=3)
 model.fit(samples)
 labesl = model.predict(samples) 

 Cluster labels for new samples:
 - New samples can be assigned to existing clusters 
 - k-means remembers the mean of each cluster (the centroids)
 - Finds the nearest centroid to each new sample 

 Scatter plots:
 import matplotlib.pyplot as plt 
 xs = samples[:, 0]
 ys = samples[:, 2]
 plt.scatter(xs, ys, c=labels)
 plt.show()

Inspecting clusters example with centroids:
# Import pyplot
import matplotlib.pyplot as plt

# Assign the columns of new_points: xs and ys
xs = new_points[:,0]
ys = new_points[:,1]

# Make a scatter plot of xs and ys, using labels to define the colors
plt.scatter(xs, ys, c=labels, alpha=0.5)

# Assign the cluster centers: centroids
centroids = model.cluster_centers_

# Assign the columns of centroids: centroids_x, centroids_y
centroids_x = centroids[:,0]
centroids_y = centroids[:,1]

# Make a scatter plot of centroids_x and centroids_y
plt.scatter(centroids_x, centroids_y, marker='D', s=50)
plt.show()

Evaluating a cluster:
- Can check correspondence with e.g. iris species 
- .... but what if there are no species to check against?
- Measure quality of a clustering 
- Informs choice of how many clusters to look for 

Cross tabulation with pandas:
- Cluster vs Species is a "cross-tabulation"
- Use the pandas library 
- Given the species of each sample as a list of species 

Aligning labels and species:
import pandas as pd 
df = pd.DataFrame({ 'labels': labels, 'species': species })

Crosstab of labels and species:
ct = pd.crosstab( df['labels'], df['species'] )
print(ct)

Measuring clustering quality:
- Using only smaples and their cluster labels 
- A good clustering has tight cluster 
- Samples in each cluster bunched togther 

Inertia measures clustering quality:
- Measures how spread out the clusters are (lower is better)
- Distance of each sample to centroid of its cluster 
- More clusters mean lower inertia 
- After fit(), available as attribute inertia_
- k-means attempts to minimize the inertia when choosing clusters 

How many clusters to choose?
- A good clustering has tight clusters (so low inertia) 
- .... but not too many clusters 
- Choose an "elbow" in the inertia plot 
- Where inertia begins to decrease more slowly 

How many clusters of grain?
ks = range(1, 6)
inertias = []

for k in ks:
    # Create a KMeans instance with k clusters: model
    model = KMeans(n_clusters=k)
    
    # Fit model to samples
    model.fit(samples)
    
    # Append the inertia to the list of inertias
    inertias.append(model.inertia_)
    
# Plot ks vs inertias
plt.plot(ks, inertias, '-o')
plt.xlabel('number of clusters, k')
plt.ylabel('inertia')
plt.xticks(ks)
plt.show()

Evaluating the grain clustering:
# Create a KMeans model with 3 clusters: model
model = KMeans(n_clusters=3)

# Use fit_predict to fit model and obtain cluster labels: labels
labels = model.fit_predict(samples)

# Create a DataFrame with labels and varieties as columns: df
df = pd.DataFrame({'labels': labels, 'varieties': varieties})

# Create crosstab: ct
ct = pd.crosstab(df["labels"], df["varieties"])

# Display ct
print(ct)

Transforming features for better clustering:
Feature variances:
- The wine features have a very different variances!
- Variance of a feature measures spread of its values 

StandardScaler:
- in Kmeans: feature variance = feature influence 
- StandardScaler transforms each feature to have mean 0 and variance 1
- Features are said to be "standardized"

sklearn StandardScaler:
from sklearn.preprocessing import StandardScaler 
scaler = StandardScaler() 
scaler.fit(samples) 
StandardScaler(copy=True, with_mean=True, with_std=True)
samples_scaled = scaler.transform(samples)

Similar methods for KMeans and StandardScaler:
- StandardScaler and KMeans have similar methods 
- Use fit() / transform() with StandardScaler 
- Use fit() / predict() with KMeans 

Pipelines combine multiple steps:
from sklearn.preprocessing import StandardScaler 
from sklearn.cluster import KMeans 
scaler = StandardScaler() 
kmeans = KMeans(n_clusters=3)
from sklearn.pipeline import make_pipeline 
pipeline = make_pipeline(scaler, kmeans)
pipeline.fit(samples)

sklearn preprocessing steps:
- StandardScaler is a "preprocessing" step 
- MaxAbsScaler and Normalizer are other examples 