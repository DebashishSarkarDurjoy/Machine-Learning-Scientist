Predicting blood glucose levels
import pandas as pd 
diabetes_df = pd.read_csv("diabetes.csv")

Creating feature and target variables 
X = diabetes_df.drop("glucose", axis=1).values 
y = diabetes_df["glucose"].values 
print(type(X), type(y)) #<class 'numpy.ndarray'> <class 'numpy.ndarray'>

Making predictions from a single feature 
X_bmi = X[:, 3] # taking the bmi column
print(y.shape, X_bmi.shape) # (752,) (752,)
X_bmi = X_bmi.reshape(-1,1) #we need 2d array for the feature. Fine for y 
print(X_bmi) #(752, 1)

Plotting glucose vs body mass index 
import matplotlib.pyplot as plt 
plt.scatter(X_bmi, y)
plt.ylabel("Blood Glucose (mg/dl)")
plt.xlabel("Body Mass Index")
plt.show()

Fitting a regression model 
from sklearn.linear_model import LinearRegression
reg = LinearRegression()
reg.fit(X_bmi, y)
predictions = reg.predict(X_bmi)
plt.scatter(X_bmi, y)
plt.plot(X_bmi, predictions)
plt.ylabel("Blood Glucose Level (md/dl)")
plt.xlabel("Body Mass Index")
plt.show()

Regression Mechanics:
- y = ax + b
- Simple linear regression uses one feature
    - y = target 
    - x = single feature 
    - a, b = parameters/coefficients of the model - slope, intercept
- How do we choose a and b?
    - Define an error function for any given line 
    - Choose the line that minimizes the error function 
- Error function = loss function = cost function 

The loss function:
- Residual = vertical distance between the predicted line and actual point 
- Ordinary Least Squares (RSS): minimize RSS.

Linear Regression in Higher dimensions:
- y = a1x1 + a2x2 + b
- we need to specify 3 variables here a1, a2, b 
- In higher dimensions:
    - Known as multiple regression
    - Must specify coefficient for each feature and the variable b 

Linear regression using all features:
from sklearn.model_selection import train_test_split 
from sklearn.linear_model import LinearRegression 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
reg_all = LinearRegression()
reg_all.fit(X_train, y_train)
y_pred = reg_all.predict(X_test)

R-squared:
- R-squared: quantifies the variance in target values explained by the features 
- Values range from 0 to 1

R-squared in scikit-learn:
reg_all.score(X_test, y_test)

Mean squared error and root mean squared error:
- MSE and RMSE 

RMSE in scikit-learn:
from sklearn.metrics import mean_squared_error
mean_squared_error(y_test, y_pred, squared=False)

Cross-validation motivation:
- Model performance is dependent on the way we split up the data 
- Not representative of the model's ability to generalize to unseen data 
- Solution: Cross-validation!

Cross-validation and model performance:
- 10 folds = 10-fold CV 
- k folds = k-fold CV 
- More folds = More computationally expensive

Cross-validation in scikit-learn:
from sklearn.model_selection import cross_val_score, KFold
kf = KFold(n_splits=6, shuffle=True, random_state=42)
reg = LinearRegression()
cv_results = cross_val_score(reg, X, y, cv=kf) # score retured is R-squared

Evaluating cross-validation performance:
print(cv_results) 
print(np.mean(cv_results), np.std(cv_results))
print(np.quantile(cv_results, [0.025, 0.975]))

Regularized regression (Ridge):
- Recall: Linear regression minimizes a loss function 
- It chooses a coefficient, a, for each feature variable, plus b 
- Large coefficients can lead to overfitting 
- Regularisation: Penalize large coefficients 

Ridge Regression (Ridge):
- Loss function = OLS loss function + alpha * summation of ai^2
- Ridge penalizes large positive or negative coefficients 
- aplha: parameter we need to choose 
- Picking alpha is similar to picking k in KNN
- Hyperparameter: variable used to optimise model parameters 
- alpha = 0 = OLS (Can lead to overfitting)
- Very high alpha: Can lead to underfitting 

Ridge regression in scikit-learn:
from sklearn.linear_model import Ridge 
scores = []
for alpha in [0.1, 1.0, 10.0, 100.0, 1000.0]:
    ridge = Ridge(alpha=alpha)
    ridge.fit(X_train, y_train)
    y_pred = ridge.predict(X_test)
    scores.append(ridge.score(X_test, y_test))
print(scores)

Lasso Regression:
- Loss function = OLS loss function + alpha * summation of |ai|

Lasso Regression in scikit-learn:
from sklearn.linear_model import Lasso 
scores = []
for alpha in [0.01, 1.0, 10.0, 20.0, 50.0]:
    lasso = Lasso(alpha=alpha)
    lasso.fit(X_train, y_train)
    lasso_pred = lasso.predict(X_test)
    scores.append(lasso.score(X_test, y_test))
print(scores)

Lasso regression for feature selection:
- Lasso can select important features of a dataset
- Shrinks the coefficients of less important features to zero
- Features not shrunk by zero are selected by lasso 

Lasso for feature selection in scikit-learn:
from sklearn.linear_model import Lasso 
X = diabeted_df.drop("glucose", axis=1).values 
y = diabetes_df["glucose"].values 
names = diabetes_df.drop("glucose", axis=1).columns
lasso = Lasso(alpha=0.1)
lasso_coef = lasso.fit(X, y).coef_
plt.bar(names, lasso_coef)
plt.xticks(rotation=45)
plt.show()

How good is your model?
Classification metrics:
- Measuring model performance with accuracy:
    - Fraction of correctly classified samples
    - Not always a useful metric 

Class imbalance:
- Classification for predicting fraudulent bank transactions
    - 99% of transactions are legitimate; 1% are fraudulent 
- Could build a classifier that predicts NONE of the transactions are fraudulent 
    - 99% accurate
    - But terrible at actually predicting fraudulent transactions 
    - Fails at its original purpose
- Class imbalance: Uneven frequency of classes 
- Need a different way to access performance 

Confusion matrix for assessing classification performance:
- Accuracy: Proportions of correct classifications (true positives and negatives) from overall number of cases 
- Accuracy: (tp + tn) / (tp + tn + fp + fn)
- Recall: Proportion of correct positive classifications (true positives) from cases that are actually positive 
- Recall(Sensitivity): (true positives) / (true positives + false negatives)
- High precision = lower false positive rate 
- High precision: Not many legitimate transactions are predicted to be fraudulent 
- High recall = lower false positive rate 
- High recall: Predicted most fraudulent transactions correctly 
- Precision(Specificity): Proportion of correct positive classification (true positives) from cases that are predicted as positive
- Precision: (true positives) / (true positives + false positives)
- F1 score: 2 * ( (precision * recall) / (precision + recall) )
- F1 score is the harmonics mean of precision and recall. It gives equal weight to precision and recall.
    Therefore it factors in both the number of errors mode by the model and the type of errors.

Confusion matrix in scikit-learn:
from sklearn.metrics import classification_report, confusion_matrix
knn = KNeighborsClassifier(n_neighbors = 7)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)
print( confusion_matrix(y_test, y_pred) )
print( classification_report(y_test, y_pred) )

Logistic regressiong for binary classification:
- Logistic regression is used to classification problems
- Logistic regresion outputs probabilities

Logistic regresion in scikit-learn:
from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression()
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
logreg.fit(X_train, y_train)
y_pred = logreg.predict(X_test)

Predicting probabilities:
y_pred_probs = logreg.predict_proba(X_test)[:, 1]

Probability thresholds:
- By default, logistic regression threshold = 0.5
- Not specific to logistic regression 
    - KNN classifiers also have thresholds 

What happens when we alter the threshold?
ROC (Receiver Operating Characteristic) curve:
- visualises how different thresholds affect true positive and false positive rates 

Plotting the ROC curve:
from sklearn.metrics import roc_curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred_probs)
plt.plot([0,1], [0,1], 'k--')
plt.plot(fpr, tpr)
plt.xlabel('False positive rate')
plt.ylabel('True positive rate')
plt.title('Logistic Regression ROC curve')
plt.show()

ROC AUC in scikit-learn:
from sklearn.metrics import roc_auc_score
print(roc_auc_score(y_test, y_pred_probs))

Hyperparameter tuning:
- Ridge/lasso regression: Choosing alpha 
- KNN: Choosing n_neighbors 
- Hyperparameters: Parameters we specify before fitting the model 
    - like alpha and n_neighbors 

Choosing the correct hyperparameters:
1. Try lots of different hyperparameter values 
2. Fit all of them separately 
3. See how well they perform 
4. Choose the best performing values 
- This is called hyperparameter tuning 
- It is essential to use cross-validation to avoid overfitting to the test set 
- We can still split the data and perform corss-validation on the training set 
- We withhold the test set for final evaluation

One approach for hyperparameter tuning is called Grid search
from sklearn.model_selection import GridSearchCV 
kf = KFold(n_splits=5, shuffle=True, random_state=42)
param_grid = { "alpha": np.arange(0.0001, 1, 10), "solver": ["sag", "lsqr"] }
ridge = Ridge()
ridge_cv = GridSearchCV(ridge, param_grid, cv=kf)
ridge_cv.fit(X_train, y_train)
print(ridge_cv.best_params_, ridge_cv.best_score_)

Limitations and alternative approach:
- 3-fold cross-validation, 1 hyperparameter, 10 total values = 30 fits
- 10 fold cross-validation, 3 hyperparameters, 30 total values = 900 fits 

RandomizedSearchCV:
- It tests a fixed number of hyperparameter settings from a specified probability distributions

RandomizedSearchCV:
from sklearn.model_selection import RandomizedSearchCV 
kf = KFold(n_splits=5, shuffle=True, random_state=42)
param_grid = { "alpha": np.arange(0.0001, 1, 10), "solver": ["sag", "lsqr"] }
ridge = Ridge()
ridge_cv = RandomizedSearchCV(ridge, param_gird, cv=kf, n_iter=2)
ridge_cv.fit(X_train, X_test)
print(ridge_cv.best_params_, ridge_cv.best_score_)

Evaluating on the test set:
test_score = ridge_cv.score(X_test, y_test)
print(test_score)

RandomizedSearchCV example:
# Create the parameter space
params = {"penalty": ["l1", "l2"],
         "tol": np.linspace(0.0001, 1.0, 50),
         "C": np.linspace(0.1, 1, 50),
         "class_weight": ["balanced", {0:0.8, 1:0.2}]}

# Instantiate the RandomizedSearchCV object
logreg_cv = RandomizedSearchCV(logreg, params, cv=kf)

# Fit the data to the model
logreg_cv.fit(X_train, y_train)

# Print the tuned parameters and score
print("Tuned Logistic Regression Parameters: {}".format(logreg_cv.best_params_))
print("Tuned Logistic Regression Best Accuracy Score: {}".format(logreg_cv.best_score_))

Preprocessing data:
Dealing with categorical data:
- scikit-learn will not accept categorical features by default 
- Need to convert categorical features into numeric values 
- Convert to binary features called dummy variables:
 - 0: Observation was not that category
 - 1: Observation was that category 

 Dealing with categorical features in Python:
 - scikit-learn: OneHotEncoder()
 - pandas: get_dummies()

 Music dataset:
 - popularity: Target variable 
 - genre: Categorical feature 

 Encoding dummy variables:
 import pandas as pd 
 music_df = pd.read_csv('music.csv")
 music_dummies = pd.get_dummies(music_df["genre"], drop_first=True)
 print(music_dummies.head())
 music_dummies = pd.concat([music_df, music_dummies], axis=1)
 music_dummies = music_dummies.drop("genre", axis=1)

Encoding dummy variables:
music_dummies = pd.get_dummies(music_df, drop_first=True)
print(music_dummies.columns)

Linear Regression with dummy variables:
from sklearn.model_selection import cross_val_score, KFold 
from sklearn.linear_model import LinearRegression 
X = music_dummies.drop("popularity", axis=1).values 
y = music_dummies["popularity"].values 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
kf = KFold(n_splits=5, shuffle=True, random_state=42)
linreg = LinearRegression()
linreg_cv = cross_val_score(linreg, X_train, y_train, cv=kf, scoring="neg_mean_squared_error")
print(np.sqrt(-linreg_cv))

Handling Missing Data:
Missing Data:
- Music Dataset: print(music.isna().sum().sort_values())

Dropping missing data (occuring less than 5%):
music_df = music_df.dropna(subset=["genre", "popularity", "loudness", "liveness", "tempo"])
print(music_df.isna().sum().sort_values())

Imputting values:
- Imputation: use subject matter expertise to replace missing data with educated guesses
- Common to use mean 
- Can also use the median or another value 
- For categorical values, we typically use the most frequent value - the mode 
- Must split our data first, to avoid leakage 

Imputation with scikit-learn:
from sklearn.impute import SimpleImputer
X_cat = music_df["genre"].values.reshape(-1,1)
X_num = music_df.drop(["genre", "popularity"], axis=1).values 
y = music_df["popularity"].values 
X_train_cat, X_test_cat, y_train, y_test = train_test_split(X_cat, y, test_size=0.2, random_state=12)
X_train_num, X_test_num, y_train, y_test = train_test_split(X_num, y, test_size=0.2, random_state=12)
imp_cat = SimpleImputer(strategy="most_frequent") #for the categorical data
X_train_cat = imp_cat.fit_transform(X_train_cat)
X_test_cat = imp_cat.transform(X_test_cat)
#for numerical data 
imp_num = SimpleImputer() # default - mean 
X_train_num = imp_num.fit_transform(X_train_num)
X_test_num = imp_num.transform(X_test_num)
X_train = np.append(X_train_num, X_train_cat, axis=1)
X_test = np.append(X_test_num, X_test_cat, axis=1)

Imputing within pipeline:
from sklearn.pipeline import Pipeline 
music_df = music_df.dropna(subset=["genre", "popularity", "loudness", "liveness", "tempo"])
music_df["genre"] = np.where(music_df["genre"] == "Rock", 1, 0)
X = music_df.drop("genre", axis=1).values 
y = music_df["genre"].values 

#Building a pipeline
steps = [ ("imputation", SimpleImputer()), ("logistic_regression", LogisticRegression()) ]
#each step but the last should be a transformer
pipeline = Pipeline(steps)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
pipeline.fit(X_train, y_train)
pipeline.score(X_test, y_test)

Centering and Scaling:
How to scale our data:
- Subtract the mean and divide by variance 
    - All features are centered around zero and have a variance of one 
    - This is called standardization
- Can also subtract the minimum and divide by the range 
    - Minimum zero and maximum one 
- Can also normalize so that data ranges from -1 to +1

Scaling in scikit-learn:
from sklearn.preprocessing import StandardScaler
X = music_df.drop("genre", axis=1).values 
y = music_df["genre"].values 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
print(np.mean(X), np.std(X))
print(np.mean(X_train_scaled), np.std(X_train_scaled))

Scaling in pipeline:
steps = [ ('scaler': StandardScaler()), ('knn': KNeighborsClassfifier(n_neighbors=6)) ]
pipeline = Pipeline(steps)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21)
knn_scaled = pipeline.fit(X_train, y_train)
y_pred = knn_scaled.predict(X_test)
print(knn_scaled.score(X_test, y_test)) # 0.81

Comparing performance using unscaled data:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21)
knn_unscaled = KNeighborsClassifer(n_neighbors=6).fit(X_train, y_train)
print(Knn_unscaled.score(X_test, y_test)) # 0.53

CV and Scaling in a pipeline:
from sklearn.model_selection import GridSearchCV 
steps = [ ('scaler', StandardScaler()), ('knn', KNeighborsClassifier()) ]
pipeline = Pipeline(steps)
parameters = { "knn__n_neighbors": np.arange(1, 50) }
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21)
cv = GridSearch(pipeline, param_grid=parameters)
cv.fit(X_train, y_train)
y_pred = cv.predict(X_test)
#checking model performance 
print(cv.best_score_) # 0.8199999
print(cv.best_params_) # {'knn_n_neighbors': 12}

Exmaple using lasso:
# Import StandardScaler
from sklearn.preprocessing import StandardScaler

# Create pipeline steps
steps = [("scaler", StandardScaler()),
         ("lasso", Lasso(alpha=0.5))]

# Instantiate the pipeline
pipeline = Pipeline(steps)
pipeline.fit(X_train, y_train)

# Calculate and print R-squared
print(pipeline.score(X_test, y_test))

Exmaple using CV and logreg:
# Build the steps
steps = [("scaler", StandardScaler()),
         ("logreg", LogisticRegression())]
pipeline = Pipeline(steps)

# Create the parameter space
parameters = {"logreg__C": np.linspace(0.001, 1.0, 20)}
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, 
                                                    random_state=21)

# Instantiate the grid search object
cv = GridSearchCV(pipeline, param_grid=parameters)

# Fit to the training data
cv.fit(X_train, y_train)
print(cv.best_score_, "\n", cv.best_params_)

Evaluating multiple models
Different models for different problems:
Some guiding principles:
- Size of the dataset:
    - Fewer features = simpler model, faster training time 
    - Some models require large amounts of data to perform well 

- Interpretability:
    - Some models are easier to explain, which can be important for stakeholder
    - Liner regression has high interpretability, as we can understand the coefficients 

- Flexibility:
    - May imporve accuracy, by making fewer assumptions about data 
    - KNN is a more flexible model, doesn't assume any linear relationships 

It's all in the metrics:
- Regression model performance:
    - RMSE 
    - R-squared 
- Classification model performance:
    - Accuracy 
    - Confusion matrix 
    - Precision, recall, F1-score 
    - ROC AUC 
- Train several models and evaluate performance out of the box

A note on scaling:
- KNN 
- Linear Regression (plus Ridge, Lasso)
- Logistic Regression 
- Artificial Neural Network

--Best to scale our data before evaluating models 

Evaluating classification models:
import matplotlib.pyplot as plt 
from sklearn.preprocessing import StandardScaler 
from sklearn.model_selection import cross_val_score, KFold, train_test_split 
from sklearn.neighbors import KNeighborsClassifier 
from sklearn.linear_model import LogisticRegression 
from sklearn.tree import DecisionTreeClassifier 

X = music.drop("genre", axis=1).values 
y = music["genre"].values 

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)
scaler = StandardScaler() 
X_train_scaled = scaler.fit_transform(X_train) 
X_test_scaled = scaler.transform(X_test) 

models = {"Logistic Regression": LogisticRegression(), "KNN": KNeighborsClassifier(), "Decision Tree": DecisionTreeClassifier()}
results = []

for model in models.values():
    kf = KFold(n_splits=6, random_state=42, shuffle=True)
    cv_results = cross_val_score(model, X_train_scaled, y_train, cv=kf)
    results.append(cv_results)

plt.boxplot(results, labels=maodels.keys())
plt.show()

Test set performance:
for name, model in models.items():
    model.fit(X_train_scaled, y_train)
    test_score = model.score(X_test_scaled, y_test)
    print("{} Test set accuracy: {}".format(name, test_score))

Pipeline for predicting song popularity:
# Create steps
steps = [("imp_mean", SimpleImputer()), 
         ("scaler", StandardScaler()), 
         ("logreg", LogisticRegression())]

# Set up pipeline
pipeline = Pipeline(steps)
params = {"logreg__solver": ["newton-cg", "saga", "lbfgs"],
         "logreg__C": np.linspace(0.001, 1.0, 10)}

# Create the GridSearchCV object
tuning = GridSearchCV(pipeline, param_grid=params)
tuning.fit(X_train, y_train)
y_pred = tuning.predict(X_test)

# Compute and print performance
print("Tuned Logistic Regression Parameters: {}, Accuracy: {}".format(tuning.best_params_, tuning.score(X_test, y_test)))

