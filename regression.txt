Predicting blood glucose levels
import pandas as pd 
diabetes_df = pd.read_csv("diabetes.csv")

Creating feature and target variables 
X = diabetes_df.drop("glucose", axis=1).values 
y = diabetes_df["glucose"].values 
print(type(X), type(y)) #<class 'numpy.ndarray'> <class 'numpy.ndarray'>

Making predictions from a single feature 
X_bmi = X[:, 3] # taking the bmi column
print(y.shape, X_bmi.shape) # (752,) (752,)
X_bmi = X_bmi.reshape(-1,1) #we need 2d array for the feature. Fine for y 
print(X_bmi) #(752, 1)

Plotting glucose vs body mass index 
import matplotlib.pyplot as plt 
plt.scatter(X_bmi, y)
plt.ylabel("Blood Glucose (mg/dl)")
plt.xlabel("Body Mass Index")
plt.show()

Fitting a regression model 
from sklearn.linear_model import LinearRegression
reg = LinearRegression()
reg.fit(X_bmi, y)
predictions = reg.predict(X_bmi)
plt.scatter(X_bmi, y)
plt.plot(X_bmi, predictions)
plt.ylabel("Blood Glucose Level (md/dl)")
plt.xlabel("Body Mass Index")
plt.show()

Regression Mechanics:
- y = ax + b
- Simple linear regression uses one feature
    - y = target 
    - x = single feature 
    - a, b = parameters/coefficients of the model - slope, intercept
- How do we choose a and b?
    - Define an error function for any given line 
    - Choose the line that minimizes the error function 
- Error function = loss function = cost function 

The loss function:
- Residual = vertical distance between the predicted line and actual point 
- Ordinary Least Squares (RSS): minimize RSS.

Linear Regression in Higher dimensions:
- y = a1x1 + a2x2 + b
- we need to specify 3 variables here a1, a2, b 
- In higher dimensions:
    - Known as multiple regression
    - Must specify coefficient for each feature and the variable b 

Linear regression using all features:
from sklearn.model_selection import train_test_split 
from sklearn.linear_model import LinearRegression 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
reg_all = LinearRegression()
reg_all.fit(X_train, y_train)
y_pred = reg_all.predict(X_test)

R-squared:
- R-squared: quantifies the variance in target values explained by the features 
- Values range from 0 to 1

R-squared in scikit-learn:
reg_all.score(X_test, y_test)

Mean squared error and root mean squared error:
- MSE and RMSE 

RMSE in scikit-learn:
from sklearn.metrics import mean_squared_error
mean_squared_error(y_test, y_pred, squared=False)

Cross-validation motivation:
- Model performance is dependent on the way we split up the data 
- Not representative of the model's ability to generalize to unseen data 
- Solution: Cross-validation!

Cross-validation and model performance:
- 10 folds = 10-fold CV 
- k folds = k-fold CV 
- More folds = More computationally expensive

Cross-validation in scikit-learn:
from sklearn.model_selection import cross_val_score, KFold
kf = KFold(n_splits=6, shuffle=True, random_state=42)
reg = LinearRegression()
cv_results = cross_val_score(reg, X, y, cv=kf) # score retured is R-squared

Evaluating cross-validation performance:
print(cv_results) 
print(np.mean(cv_results), np.std(cv_results))
print(np.quantile(cv_results, [0.025, 0.975]))

Regularized regression (Ridge):
- Recall: Linear regression minimizes a loss function 
- It chooses a coefficient, a, for each feature variable, plus b 
- Large coefficients can lead to overfitting 
- Regularisation: Penalize large coefficients 

Ridge Regression (Ridge):
- Loss function = OLS loss function + alpha * summation of ai^2
- Ridge penalizes large positive or negative coefficients 
- aplha: parameter we need to choose 
- Picking alpha is similar to picking k in KNN
- Hyperparameter: variable used to optimise model parameters 
- alpha = 0 = OLS (Can lead to overfitting)
- Very high alpha: Can lead to underfitting 

Ridge regression in scikit-learn:
from sklearn.linear_model import Ridge 
scores = []
for alpha in [0.1, 1.0, 10.0, 100.0, 1000.0]:
    ridge = Ridge(alpha=alpha)
    ridge.fit(X_train, y_train)
    y_pred = ridge.predict(X_test)
    scores.append(ridge.score(X_test, y_test))
print(scores)

Lasso Regression:
- Loss function = OLS loss function + alpha * summation of |ai|

Lasso Regression in scikit-learn:
from sklearn.linear_model import Lasso 
scores = []
for alpha in [0.01, 1.0, 10.0, 20.0, 50.0]:
    lasso = Lasso(alpha=alpha)
    lasso.fit(X_train, y_train)
    lasso_pred = lasso.predict(X_test)
    scores.append(lasso.score(X_test, y_test))
print(scores)

Lasso regression for feature selection:
- Lasso can select important features of a dataset
- Shrinks the coefficients of less important features to zero
- Features not shrunk by zero are selected by lasso 

Lasso for feature selection in scikit-learn:
from sklearn.linear_model import Lasso 
X = diabeted_df.drop("glucose", axis=1).values 
y = diabetes_df["glucose"].values 
names = diabetes_df.drop("glucose", axis=1).columns
lasso = Lasso(alpha=0.1)
lasso_coef = lasso.fit(X, y).coef_
plt.bar(names, lasso_coef)
plt.xticks(rotation=45)
plt.show()